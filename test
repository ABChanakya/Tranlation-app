from gensim.models import Word2Vec
from gensim.test.utils import common_texts
import gensim.downloader as api
from collections import Counter
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE

# Set random seeds
np.random.seed(42)
torch.manual_seed(42)

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Script is running on: {device}")

# 1. Train Word2Vec on common_texts
gensim_model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)

# 2. Load Pretrained Model
model2 = api.load('word2vec-google-news-300')
print(model2.most_similar(['king', 'man', 'woman'], topn=10))
print("\n this just blocker stuff\n")
print(model2.most_similar(['france', 'paris', 'athens'], topn=10))
print("\n this just blocker stuff\n")
print(model2.most_similar(['Doctor', 'man', 'woman', 'Nurse'], topn=10))

# 3. t-SNE Visualization
def tsne_scatterplot(model, search_word, topn):
    sns.set_style('whitegrid')
    sns.set_theme(rc={'figure.figsize': (11.7, 8.27)})

    labels = [search_word]
    tokens = [model[search_word]]
    similar = [1]
    close_words = model.similar_by_word(search_word, topn=topn)
    for word in close_words:
        tokens.append(model[word[0]])
        labels.append(word[0])
        similar.append(word[1])

    tsne_model = TSNE(n_components=2, perplexity=topn, init='pca', random_state=42)
    coordinates = tsne_model.fit_transform(np.array(tokens))
    df = sns.scatterplot(x=coordinates[:, 0], y=coordinates[:, 1], hue=labels, palette='Reds', legend=False)
    for i in range(len(labels)):
        plt.annotate(f"{labels[i]} ({similar[i]:.2f})", (coordinates[i, 0], coordinates[i, 1]))
    plt.title(f't-SNE visualization for "{search_word}"')
    plt.show()

# Example visualization
tsne_scatterplot(model2, "king", 10)

# 4. Prepare Corpus
corpus = api.load('text8')
min_freq = 5
context_size = 5
vocab_size = 2**15

word_count = Counter()
for sentence in tqdm(corpus, desc='Counting words'):
    word_count.update(sentence)

most_common = word_count.most_common(vocab_size - 1)
word2idx = {word: i+1 for i, (word, _) in enumerate(most_common)}
word2idx['<unk>'] = 0
idx2word = {i: w for w, i in word2idx.items()}

indexed_corpus = [[word2idx.get(word, 0) for word in sentence] for sentence in corpus]

pairs = []
for sentence in tqdm(indexed_corpus, desc="Building pairs"):
    for i, center in enumerate(sentence):
        for j in range(max(0, i - context_size), min(len(sentence), i + context_size + 1)):
            if i != j:
                pairs.append((center, sentence[j]))

class Word2VecDataset(Dataset):
    def __init__(self, pairs):
        self.pairs = pairs

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        return torch.tensor(self.pairs[idx][0]), torch.tensor(self.pairs[idx][1])

dataset = Word2VecDataset(pairs)

# 5. Define SkipGram Model
class SkipGramModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(SkipGramModel, self).__init__()
        self.center_embed = nn.Embedding(vocab_size, embedding_dim)
        self.context_embed = nn.Embedding(vocab_size, embedding_dim)

    def forward(self, center_words, context_words):
        center_vectors = self.center_embed(center_words)
        context_vectors = self.context_embed(context_words)
        dot_products = (center_vectors * context_vectors).sum(dim=1)
        return dot_products

# 6. Training Loop
VOCAB_SIZE = len(word2idx)
EMBEDDING_DIM = 32
model = SkipGramModel(VOCAB_SIZE, EMBEDDING_DIM).to(device)
optimizer = optim.Adam(model.parameters(), lr=0.003)
loss_fn = nn.BCEWithLogitsLoss()
dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)

NUM_EPOCHS = 5
model.train()
for epoch in range(NUM_EPOCHS):
    total_loss = 0
    print(f"\nüîÅ Epoch {epoch+1}/{NUM_EPOCHS}")
    loop = tqdm(dataloader, total=len(dataloader), desc=f"Epoch {epoch+1}", leave=False)

    for center, context in loop:
        center = center.to(device).long()
        context = context.to(device).long()
        labels = torch.ones(center.size(0)).to(device)

        preds = model(center, context)
        loss = loss_fn(preds, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        loop.set_postfix(loss=loss.item())

    print(f"‚úÖ Epoch {epoch+1} completed ‚Äî Avg Loss: {total_loss:.4f}")

# 7. Evaluate embeddings
def find_nearest(word, topk=5):
    idx = word2idx.get(word, 0)
    word_vec = model.center_embed(torch.tensor([idx]).to(device))
    word_vec = F.normalize(word_vec, dim=1)
    all_vecs = F.normalize(model.center_embed.weight, dim=1)
    sims = torch.matmul(word_vec, all_vecs.T).squeeze()
    topk_idx = sims.topk(topk+1).indices[1:]
    return [idx2word[i.item()] for i in topk_idx]

print("Similar to 'king':", find_nearest("king"))

print("Script finished running.")